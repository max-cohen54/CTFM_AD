{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59564e3-81aa-4c38-b651-c9025d0901f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditioning on the particle type, not masking the velocity field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ba566-efc1-4925-9618-82ba6655cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torchdyn.core import NeuralODE\n",
    "from torchcfm.conditional_flow_matching import ExactOptimalTransportConditionalFlowMatcher\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "import load_and_preprocess as lap\n",
    "import train_and_eval_functions as taef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09190118-edbe-4c5a-a39f-e04cca9e22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = lap.load_and_preprocess(plots_path='/eos/home-m/mmcohen/ctfm_development/trained_models/trial_1/plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff6623-8b32-40a7-ba0a-1d03ef0192d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerVectorField(nn.Module):\n",
    "    def __init__(self, input_dim=3, model_dim=128, num_heads=8, num_layers=4, \n",
    "                 n_mask_vals=5, ff_dim=512):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        # project the 3D features\n",
    "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "\n",
    "        # embed mask (0=pad, 1-4=particle types)\n",
    "        self.mask_emb = nn.Embedding(n_mask_vals, model_dim)\n",
    "\n",
    "        # time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, model_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(model_dim, model_dim)\n",
    "        )\n",
    "\n",
    "        # transformer stack\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # back to velocity\n",
    "        self.output_proj = nn.Linear(model_dim, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fef48-bee8-4fe9-9f7a-41b3118ab245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device, num_epochs=50, sigma=0.0):\n",
    "    model.to(device)\n",
    "    # flow matcher does not support conditioning directly; we embed and mask slots instead\n",
    "    FM = ExactOptimalTransportConditionalFlowMatcher(sigma=sigma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.\n",
    "        for i, (x1, mask) in enumerate(tqdm(dataloader)):\n",
    "            x1 = x1.to(device)\n",
    "            mask = mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # sample noise\n",
    "            x0 = torch.randn_like(x1)\n",
    "            # sample flow-matching tuples with condition\n",
    "            t, xt, ut = FM.sample_location_and_conditional_flow(x0, x1)\n",
    "\n",
    "            # model prediction\n",
    "            vt = model(t, xt, mask=mask)\n",
    "            \n",
    "            # Compute loss over all dimensions\n",
    "            sq_err = (vt - ut)**2                      # [B, N, D]\n",
    "            loss = torch.mean(sq_err)                  # Mean over all dimensions\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} - Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8acab2-b9a8-4f5a-a11b-9e639299f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset = EvtDataset(sel_train[:ntrain].astype(np.float32))\n",
    "dataloader = DataLoader(dataset, batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "model = TransformerVectorField(input_dim=input_dim, model_dim=model_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "if not trained:\n",
    "    train(model, dataloader, optimizer, device=device, num_epochs=numberOfEpochs, sigma=flowSigma)\n",
    "    \n",
    "    # Assuming `node` is your trained NeuralODE (e.g., torchcfm.NeuralODE)\n",
    "    torch.save({\n",
    "        'state_dict': model.state_dict(),\n",
    "        'model_kwargs': {\n",
    "            'input_dim': input_dim,\n",
    "            'model_dim': model_dim,\n",
    "            'num_heads': num_heads,\n",
    "            'num_layers': num_layers\n",
    "        }\n",
    "    }, \"cond_no_mask_flow_model_%s.pt\"%label)\n",
    "\n",
    "else:\n",
    "    # Rebuild the vector field\n",
    "    ckpt = torch.load(\"cond_no_mask_flow_model_%s.pt\"%label, map_location=device)\n",
    "    \n",
    "    # Recreate the architecture\n",
    "    model = TransformerVectorField(**ckpt['model_kwargs'])\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    model.to(device).eval()\n",
    "\n",
    "node = NeuralODE(model, solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270668b-7b72-4737-9095-757e699a10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# animate flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2afe5a-6e1b-420d-aca6-883a53ae1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_base_distribution(node, x1, mask, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Maps x1 (data space) to x0 (base space) using the inverse flow in batches.\n",
    "    Applies mask to ignore padded particles.\n",
    "    \n",
    "    Args:\n",
    "        node: A trained NeuralODE.\n",
    "        x1: [B, N, D] torch.Tensor - data space samples.\n",
    "        mask: [B, N, 1] torch.Tensor - binary mask for valid particles.\n",
    "        batch_size: int - batch size for processing.\n",
    "\n",
    "    Returns:\n",
    "        x0: [B, N, D] torch.Tensor - mapped base samples with mask applied.\n",
    "    \"\"\"\n",
    "    t_span = torch.linspace(1., 0., 2, device=x1.device)\n",
    "    outputs = []\n",
    "\n",
    "    for i in tqdm(range(0, x1.size(0), batch_size)):\n",
    "        xb = x1[i:i+batch_size]\n",
    "        mb = mask[i:i+batch_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(xb, t_span=t_span)  # [2, bsz, N, D]\n",
    "            x0_b = traj[-1]  # No mask applied here\n",
    "\n",
    "        outputs.append(x0_b)\n",
    "\n",
    "    return torch.cat(outputs, dim=0)\n",
    "\n",
    "test_evts_tensor = torch.tensor(parts_test, dtype=torch.float32).to(device)\n",
    "\n",
    "ntest = 2000000\n",
    "\n",
    "# Extract features and mask\n",
    "tp = np.random.permutation(test_evts_tensor.shape[0])\n",
    "x_test = test_evts_tensor[tp[:ntest], :, :3]       # [eta, phi, pt]\n",
    "mask_test = test_evts_tensor[tp[:ntest], :, 3:]     # [mask], shape [B, N, 1]\n",
    "\n",
    "x0_latent = map_to_base_distribution(node, x_test, mask_test)\n",
    "\n",
    "print(\"Mapped shape:\", x0_latent.shape)  # Should be [B, maxparts, 3]\n",
    "print(\"Latent mean (should be ~0):\", x0_latent.mean().item())\n",
    "print(\"Latent std (should be ~1):\", x0_latent.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a19f1e-bc96-4bf6-a4cd-f9796e3136a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features post flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33032d95-a049-4251-ad0f-451eb8886a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_prob_gaussian(x0, mask=None):\n",
    "    \"\"\"\n",
    "    Compute -log p(x0) under a standard N-dimensional Gaussian,\n",
    "    over all dimensions including padded particles.\n",
    "\n",
    "    Args:\n",
    "        x0: [B, N, D] tensor\n",
    "        mask: [B, N, 1] binary mask tensor (optional, not used)\n",
    "\n",
    "    Returns:\n",
    "        nll: [B] tensor, negative log-likelihood per jet\n",
    "    \"\"\"\n",
    "    # Standard Gaussian: log p(x) = -0.5 * (x^2 + log(2Ï€))\n",
    "    log_probs = -0.5 * (x0 ** 2 + torch.log(torch.tensor(2 * torch.pi, device=x0.device)))\n",
    "\n",
    "    # Sum over all particles and features without masking\n",
    "    log_likelihood = log_probs.sum(dim=(1, 2))  # [B]\n",
    "\n",
    "    # Return negative log-likelihood\n",
    "    return -log_likelihood  # [B]\n",
    "\n",
    "nll = neg_log_prob_gaussian(x0_latent, mask_test).cpu().numpy()  # [B]\n",
    "print(\"Mean NLL per jet:\", np.mean(nll))\n",
    "\n",
    "labelnames = ['bkg', 'a4l', 'htaunu', 'htautau', 'lq']\n",
    "for i in range(5):\n",
    "    _,bins,_ = plt.hist(nll[labels_test[tp[:ntest],i]==1],bins=50 if i==0 else bins, histtype='step',label=labelnames[i],density=True)\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c0f1d-49fd-4659-a25c-80816612e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
