{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59564e3-81aa-4c38-b651-c9025d0901f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditioning on the particle type, masking the velocity field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b7aacc-4d8e-4086-b42d-9ce3aae4ee89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchcfm\n",
      "  Downloading torchcfm-1.0.7-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchcfm) (2.3.1)\n",
      "Requirement already satisfied: matplotlib in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchcfm) (3.8.3)\n",
      "Requirement already satisfied: numpy in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchcfm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchcfm) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchcfm) (1.5.1)\n",
      "Collecting torchdyn>=1.0.6 (from torchcfm)\n",
      "  Downloading torchdyn-1.0.6-py3-none-any.whl.metadata (891 bytes)\n",
      "Collecting pot (from torchcfm)\n",
      "  Downloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting torchdiffeq (from torchcfm)\n",
      "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
      "Requirement already satisfied: absl-py in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchcfm) (1.4.0)\n",
      "Requirement already satisfied: pandas>=2.2.2 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchcfm) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from pandas>=2.2.2->torchcfm) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from pandas>=2.2.2->torchcfm) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from pandas>=2.2.2->torchcfm) (2024.1)\n",
      "Requirement already satisfied: filelock in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torch>=1.11.0->torchcfm) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torch>=1.11.0->torchcfm) (4.12.1)\n",
      "Requirement already satisfied: sympy in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torch>=1.11.0->torchcfm) (1.12)\n",
      "Requirement already satisfied: networkx in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torch>=1.11.0->torchcfm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torch>=1.11.0->torchcfm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torch>=1.11.0->torchcfm) (2024.6.1)\n",
      "Requirement already satisfied: ipykernel in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchdyn>=1.0.6->torchcfm) (6.0.2)\n",
      "Requirement already satisfied: ipywidgets in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchdyn>=1.0.6->torchcfm) (7.6.3)\n",
      "Collecting poethepoet<0.11.0,>=0.10.0 (from torchdyn>=1.0.6->torchcfm)\n",
      "  Downloading poethepoet-0.10.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pytorch-lightning in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchdyn>=1.0.6->torchcfm) (2.2.4)\n",
      "Collecting torchcde<0.3.0,>=0.2.3 (from torchdyn>=1.0.6->torchcfm)\n",
      "  Downloading torchcde-0.2.5-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting torchsde (from torchdyn>=1.0.6->torchcfm)\n",
      "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: torchvision in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from torchdyn>=1.0.6->torchcfm) (0.18.0a0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from matplotlib->torchcfm) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from matplotlib->torchcfm) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from matplotlib->torchcfm) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from matplotlib->torchcfm) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /eos/user/m/mmcohen/.local/lib/python3.9/site-packages (from matplotlib->torchcfm) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from matplotlib->torchcfm) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from matplotlib->torchcfm) (2.4.7)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from scikit-learn->torchcfm) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from scikit-learn->torchcfm) (3.5.0)\n",
      "Requirement already satisfied: six in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from cycler>=0.10->matplotlib->torchcfm) (1.16.0)\n",
      "Requirement already satisfied: pastel<0.3.0,>=0.2.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from poethepoet<0.11.0,>=0.10.0->torchdyn>=1.0.6->torchcfm) (0.2.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.6.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from poethepoet<0.11.0,>=0.10.0->torchdyn>=1.0.6->torchcfm) (0.11.8)\n",
      "Collecting trampoline>=0.1.2 (from torchsde->torchdyn>=1.0.6->torchcfm)\n",
      "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from ipykernel->torchdyn>=1.0.6->torchcfm) (1.3.0)\n",
      "Requirement already satisfied: ipython<8.0,>=7.23.1 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from ipykernel->torchdyn>=1.0.6->torchcfm) (7.25.0)\n",
      "Requirement already satisfied: traitlets<6.0,>=4.1.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from ipykernel->torchdyn>=1.0.6->torchcfm) (5.9.0)\n",
      "Requirement already satisfied: jupyter_client<7.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from ipykernel->torchdyn>=1.0.6->torchcfm) (6.1.12)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from ipykernel->torchdyn>=1.0.6->torchcfm) (6.4.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from ipykernel->torchdyn>=1.0.6->torchcfm) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->torchcfm) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from pytorch-lightning->torchdyn>=1.0.6->torchcfm) (4.66.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from pytorch-lightning->torchdyn>=1.0.6->torchcfm) (6.0.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from pytorch-lightning->torchdyn>=1.0.6->torchcfm) (1.4.0.post0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from pytorch-lightning->torchdyn>=1.0.6->torchcfm) (0.8.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from sympy->torch>=1.11.0->torchcfm) (1.2.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->torchdyn>=1.0.6->torchcfm) (3.8.4)\n",
      "Requirement already satisfied: jupyter_core>=4.6.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from jupyter_client<7.0->ipykernel->torchdyn>=1.0.6->torchcfm) (4.7.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from jupyter_client<7.0->ipykernel->torchdyn>=1.0.6->torchcfm) (22.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->torchdyn>=1.0.6->torchcfm) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->torchdyn>=1.0.6->torchcfm) (2.0.6)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->torchdyn>=1.0.6->torchcfm) (6.0.2)\n",
      "Requirement already satisfied: async_timeout<5.0,>=4.0.0a3 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->torchdyn>=1.0.6->torchcfm) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->torchdyn>=1.0.6->torchcfm) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->torchdyn>=1.0.6->torchcfm) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->torchdyn>=1.0.6->torchcfm) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.0 in /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->torchdyn>=1.0.6->torchcfm) (3.2)\n",
      "Downloading torchcfm-1.0.7-py3-none-any.whl (29 kB)\n",
      "Downloading torchdyn-1.0.6-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading POT-0.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
      "Downloading poethepoet-0.10.0-py3-none-any.whl (31 kB)\n",
      "Downloading torchcde-0.2.5-py3-none-any.whl (28 kB)\n",
      "Downloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
      "\u001b[33mDEPRECATION: gosam 2.1.1-4b98559 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of gosam or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: trampoline, poethepoet, pot, torchsde, torchdiffeq, torchcde, torchdyn, torchcfm\n",
      "\u001b[33m  WARNING: The script poe is installed in '/eos/user/m/mmcohen/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed poethepoet-0.10.0 pot-0.9.5 torchcde-0.2.5 torchcfm-1.0.7 torchdiffeq-0.2.5 torchdyn-1.0.6 torchsde-0.2.6 trampoline-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --user torchcfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd73b86-9fc4-4b6f-bda0-e543257fb9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Can not combine '--user' and '--target'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3.11 -m pip install --user torchdyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1847122f-7e1f-4dc5-9c9e-81407d210b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: torchdyn\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3.11 -m pip show torchdyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "484df7bd-f044-4554-921c-4a6a3b3a3087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated sys.path:\n",
      "['/eos/user/m/mmcohen/.local/lib/python3.11/site-packages', '/eos/home-i03/m/mmcohen/ctfm_development/src/CTFM_AD/event_level_AD', '', '/eos/user/m/mmcohen/.local/lib/python3.9/site-packages', '/cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages/itk', '/cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/python', '/cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib', '/cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages', '/usr/local/lib/swan/nb_term_lib', '/cvmfs/sft.cern.ch/lcg/releases/Python/3.11.9-2924c/x86_64-el9-gcc11-opt/lib/python311.zip', '/cvmfs/sft.cern.ch/lcg/releases/Python/3.11.9-2924c/x86_64-el9-gcc11-opt/lib/python3.11', '/cvmfs/sft.cern.ch/lcg/releases/Python/3.11.9-2924c/x86_64-el9-gcc11-opt/lib/python3.11/lib-dynload', '/cvmfs/sft.cern.ch/lcg/releases/Python/3.11.9-2924c/x86_64-el9-gcc11-opt/lib/python3.11/site-packages', '/cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages/IPython/extensions', '/home/mmcohen/.ipython']\n",
      "Success! torchdyn imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "python311_user_path = '/eos/user/m/mmcohen/.local/lib/python3.11/site-packages'\n",
    "if python311_user_path not in sys.path:\n",
    "    sys.path.insert(0, python311_user_path)\n",
    "\n",
    "print(\"Updated sys.path:\")\n",
    "print(sys.path)\n",
    "\n",
    "# Now try importing\n",
    "try:\n",
    "    from torchdyn.core import NeuralODE\n",
    "    print(\"Success! torchdyn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Still can't import: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08ba566-efc1-4925-9618-82ba6655cd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:54:26.328356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "#from torchdyn.core import NeuralODE\n",
    "from torchcfm.conditional_flow_matching import ExactOptimalTransportConditionalFlowMatcher\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "import load_and_preprocess as lap\n",
    "import train_and_eval_functions as taef\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d07e002-5baa-4e95-8340-d5288f28442a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.9 (main, Jun 24 2024, 14:32:54) [GCC 11.3.0]\n",
      "PyTorch version: 2.3.1\n",
      "PyTorch file location: /cvmfs/sft.cern.ch/lcg/views/LCG_106a_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages/torch/__init__.py\n",
      "CUDA available: True\n",
      "CUDA version: 12.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch file location: {torch.__file__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5ca403d-090d-4ba7-a152-ae67a213218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: NVIDIA A100-PCIE-40GB\n",
      "GPU count: 1\n",
      "Current device: 0\n",
      "GPU memory: 42.5 GB\n",
      "GPU compute capability: 8.0\n",
      "Basic CUDA operations work fine\n",
      "Result shape: torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"GPU compute capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "\n",
    "# Test a simple CUDA operation\n",
    "try:\n",
    "    x = torch.randn(10, 10).cuda()\n",
    "    y = torch.randn(10, 10).cuda()\n",
    "    z = torch.mm(x, y)\n",
    "    print(\"Basic CUDA operations work fine\")\n",
    "    print(f\"Result shape: {z.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Basic CUDA test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0654001",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'otfm_40mhz_sigma0p01'\n",
    "\n",
    "batchSize = 512\n",
    "numberOfEpochs = 20\n",
    "patience = 5\n",
    "\n",
    "pxpypz = True\n",
    "p_train = 0.5\n",
    "p_test = 0.25\n",
    "plots_path = '/eos/home-m/mmcohen/ctfm_development/trained_models/trial_1/plots'\n",
    "\n",
    "input_dim = 3\n",
    "model_dim = 128\n",
    "ff_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "n_mask_vals = 5 # 4 particle types + 1 padding\n",
    "\n",
    "flowSigma = 0.01\n",
    "\n",
    "trained = False\n",
    "multiGPU = True\n",
    "\n",
    "\n",
    "if not multiGPU:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09190118-edbe-4c5a-a39f-e04cca9e22cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting...\n",
      "Loaded bkg\n",
      "Loaded a4l\n",
      "Loaded htaunu\n",
      "Loaded htautau\n",
      "Loaded lq\n",
      "a4l: (55969, 19, 4)\n",
      "htaunu: (760272, 19, 4)\n",
      "htautau: (691283, 19, 4)\n",
      "lq: (340544, 19, 4)\n",
      "bkg_train: (1000000, 19, 4)\n",
      "bkg_test: (1000000, 19, 4)\n",
      "bkg_val: (1000000, 19, 4)\n"
     ]
    }
   ],
   "source": [
    "datasets = lap.load_and_preprocess(\n",
    "    p_train=p_train,\n",
    "    p_test=p_test,\n",
    "    plots_path=plots_path,\n",
    "    pxpypz=pxpypz\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ff6623-8b32-40a7-ba0a-1d03ef0192d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerVectorField(nn.Module):\n",
    "    def __init__(self, input_dim=3, model_dim=128, num_heads=8, num_layers=4, \n",
    "                 n_mask_vals=5, ff_dim=512):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        # project the 3D features\n",
    "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "\n",
    "        # embed mask (0=pad, 1-4=particle types)\n",
    "        self.mask_emb = nn.Embedding(n_mask_vals, model_dim)\n",
    "\n",
    "        # time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, model_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(model_dim, model_dim)\n",
    "        )\n",
    "\n",
    "        # transformer stack\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # back to velocity\n",
    "        self.output_proj = nn.Linear(model_dim, input_dim)\n",
    "\n",
    "    def forward(self, t, x, mask=None, **kwargs):\n",
    "        # x: [B, N, 3], mask: [B, N, 1]\n",
    "        B, N, _ = x.shape\n",
    "\n",
    "        # 1) feature projection\n",
    "        h = self.input_proj(x)                          # [B, N, model_dim]\n",
    "\n",
    "        # 2) mask embedding and injection\n",
    "        if mask is not None:\n",
    "            m = mask.squeeze(-1).long()                        # [B, N]\n",
    "            m_emb = self.mask_emb(m)                    # [B, N, model_dim]\n",
    "            h = h + m_emb\n",
    "\n",
    "        # 3) time embedding\n",
    "        if isinstance(t, float) or t.ndim == 0:\n",
    "            t = torch.full((B, 1), float(t), device=x.device)\n",
    "        elif t.ndim == 1:\n",
    "            t = t.view(-1, 1)\n",
    "        time_emb = self.time_mlp(t)[:, None, :]         # [B, 1, model_dim]\n",
    "        h = h + time_emb                                # broadcast to [B, N, model_dim]\n",
    "\n",
    "        # 4) transformer with padding mask\n",
    "        src_key_padding_mask = None\n",
    "        if mask is not None:\n",
    "            # mask==0 indicates padding positions\n",
    "            src_key_padding_mask = (mask.squeeze(-1) == 0)  # [B, N] boolean\n",
    "        h = self.transformer(h, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # 5) output velocity\n",
    "        v = self.output_proj(h)                         # [B, N, 3]\n",
    "        # zero out velocities on padded slots\n",
    "        if mask is not None:\n",
    "            binary_mask = (mask > 0).float()   # [B, N, 1]: 1 if particle exists, 0 if padded\n",
    "            v = v * binary_mask\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "740fef48-bee8-4fe9-9f7a-41b3118ab245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device, num_epochs=50, sigma=0.0):\n",
    "    model.to(device)\n",
    "    # flow matcher does not support conditioning directly; we embed and mask slots instead\n",
    "    FM = ExactOptimalTransportConditionalFlowMatcher(sigma=sigma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.\n",
    "        for i, (x1, mask) in enumerate(tqdm(dataloader)):\n",
    "            x1 = x1.to(device)\n",
    "            mask = mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # sample noise\n",
    "            x0 = torch.randn_like(x1)\n",
    "            # sample flow-matching tuples with condition\n",
    "            t, xt, ut = FM.sample_location_and_conditional_flow(x0, x1)\n",
    "\n",
    "            # model prediction\n",
    "            vt = model(t, xt, mask=mask)\n",
    "            # mask ground-truth velocity\n",
    "            binary_mask = (mask > 0).float()            # [B, N, 1]\n",
    "            ut = ut * binary_mask\n",
    "\n",
    "            # compute loss only on active slots\n",
    "            # vt, ut: [B, N, D], binary_mask: [B, N, 1]\n",
    "            mask_expanded = binary_mask.expand_as(vt)            # [B, N, D]\n",
    "            sq_err = (vt - ut)**2                                # [B, N, D]\n",
    "            loss = torch.sum(sq_err * mask_expanded)             # sum over only active dims\n",
    "            loss = loss / mask_expanded.sum().clamp(min=1)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} - Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "215d8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvtDataset(Dataset):\n",
    "    def __init__(self, evts):  # jets: [N, maxparts, 4]\n",
    "        self.x = evts[:, :, :3]           # [eta, phi, pt]\n",
    "        self.mask = evts[:, :, 3:]        # [mask]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.mask[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b8acab2-b9a8-4f5a-a11b-9e639299f80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1953 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_764/3797891013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumberOfEpochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflowSigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Assuming `node` is your trained NeuralODE (e.g., torchcfm.NeuralODE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_764/154730588.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, device, num_epochs, sigma)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# sample noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;31m# sample flow-matching tuples with condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_location_and_conditional_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset = EvtDataset(datasets['bkg_train']['data'].astype(np.float32))\n",
    "dataloader = DataLoader(dataset, batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "\n",
    "model = TransformerVectorField(input_dim=input_dim, model_dim=model_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "if not trained:\n",
    "    train(model, dataloader, optimizer, device=device, num_epochs=numberOfEpochs, sigma=flowSigma)\n",
    "    \n",
    "    # Assuming `node` is your trained NeuralODE (e.g., torchcfm.NeuralODE)\n",
    "    torch.save({\n",
    "        'state_dict': model.state_dict(),\n",
    "        'model_kwargs': {\n",
    "            'input_dim': input_dim,\n",
    "            'model_dim': model_dim,\n",
    "            'num_heads': num_heads,\n",
    "            'num_layers': num_layers\n",
    "        }\n",
    "    }, \"flow_model_%s.pt\"%label)\n",
    "\n",
    "else:\n",
    "    # Rebuild the vector field\n",
    "    ckpt = torch.load(\"flow_model_%s.pt\"%label, map_location=device)\n",
    "    \n",
    "    # Recreate the architecture\n",
    "    model = TransformerVectorField(**ckpt['model_kwargs'])\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    model.to(device).eval()\n",
    "\n",
    "node = NeuralODE(model, solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270668b-7b72-4737-9095-757e699a10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# animate flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2afe5a-6e1b-420d-aca6-883a53ae1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_base_distribution(node, x1, mask, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Maps x1 (data space) to x0 (base space) using the inverse flow in batches.\n",
    "    Applies mask to ignore padded particles.\n",
    "    \n",
    "    Args:\n",
    "        node: A trained NeuralODE.\n",
    "        x1: [B, N, D] torch.Tensor - data space samples.\n",
    "        mask: [B, N, 1] torch.Tensor - binary mask for valid particles.\n",
    "        batch_size: int - batch size for processing.\n",
    "\n",
    "    Returns:\n",
    "        x0: [B, N, D] torch.Tensor - mapped base samples with mask applied.\n",
    "    \"\"\"\n",
    "    t_span = torch.linspace(1., 0., 2, device=x1.device)\n",
    "    outputs = []\n",
    "\n",
    "    for i in tqdm(range(0, x1.size(0), batch_size)):\n",
    "        xb = x1[i:i+batch_size]\n",
    "        mb = mask[i:i+batch_size]\n",
    "        binary_mask = (mb > 0).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(xb, t_span=t_span)  # [2, bsz, N, D]\n",
    "            x0_b = traj[-1]  * binary_mask\n",
    "\n",
    "        outputs.append(x0_b)\n",
    "\n",
    "    return torch.cat(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_tags = ['bkg_val', 'bkg_train']\n",
    "for tag, data_dict in datasets.items():\n",
    "    if tag in skip_tags:\n",
    "        continue\n",
    "    \n",
    "    torch_data = torch.tensor(data_dict['data'], dtype=torch.float32).to(device)\n",
    "    torch_x = torch_data[:,:,:3]\n",
    "    torch_mask = torch_data[:,:,3:]\n",
    "\n",
    "    data_dict['x0_latent'] = map_to_base_distribution(node, torch_x, torch_mask)\n",
    "    data_dict['torch_mask'] = torch_mask\n",
    "\n",
    "    print(tag)\n",
    "    print(\"Mapped shape:\", data_dict['x0_latent'].shape)  # Should be [B, maxparts, 3]\n",
    "    print(\"Latent mean (should be ~0):\", data_dict['x0_latent'].mean().item())\n",
    "    print(\"Latent std (should be ~1):\", data_dict['x0_latent'].std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a19f1e-bc96-4bf6-a4cd-f9796e3136a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features post flow\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(datasets['bkg_test']['x0_latent'][:,:,0].flatten()[datasets['bkg_test']['x0_latent'][:,:,-1].flatten()>0.5], bins=50, histtype='step', density=True, label='feature 1')\n",
    "plt.hist(datasets['bkg_test']['x0_latent'][:,:,1].flatten()[datasets['bkg_test']['x0_latent'][:,:,-1].flatten()>0.5], bins=50, histtype='step', density=True, label='feature 2')\n",
    "plt.hist(datasets['bkg_test']['x0_latent'][:,:,2].flatten()[datasets['bkg_test']['x0_latent'][:,:,-1].flatten()>0.5], bins=50, histtype='step', density=True, label='feature 3')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'{plots_path}/post_flow_feature_histograms.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33032d95-a049-4251-ad0f-451eb8886a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_prob_gaussian(x0, mask):\n",
    "    \"\"\"\n",
    "    Compute -log p(x0) under a standard N-dimensional Gaussian,\n",
    "    masking out invalid (padded) particles.\n",
    "\n",
    "    Args:\n",
    "        x0: [B, N, D] tensor\n",
    "        mask: [B, N, 1] binary mask tensor (1 = valid, 0 = padded)\n",
    "\n",
    "    Returns:\n",
    "        nll: [B] tensor, negative log-likelihood per jet\n",
    "    \"\"\"\n",
    "    # Standard Gaussian: log p(x) = -0.5 * (x^2 + log(2π))\n",
    "    log_probs = -0.5 * (x0 ** 2 + torch.log(torch.tensor(2 * torch.pi, device=x0.device)))\n",
    "\n",
    "    # Apply mask and sum over particles and features\n",
    "    binary_mask = (mask > 0).float()\n",
    "    log_probs_masked = log_probs * binary_mask  # [B, N, D]\n",
    "    log_likelihood = log_probs_masked.sum(dim=(1, 2))  # [B]\n",
    "\n",
    "    # Return negative log-likelihood\n",
    "    return -log_likelihood  # [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc482b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag, data_dict in datasets.items():\n",
    "    if tag in skip_tags:\n",
    "        continue\n",
    "    data_dict['AD_scores'] = neg_log_prob_gaussian(data_dict['x0_latent'], data_dict['torch_mask']).cpu().numpy()  # [B]\n",
    "    print(tag)\n",
    "    print(\"Mean NLL per jet:\", np.mean(data_dict['AD_scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AD scores\n",
    "plt.figure(figsize=(12, 8))\n",
    "for tag, data_dict in datasets.items():\n",
    "    if tag in skip_tags:\n",
    "        continue\n",
    "    plt.hist(data_dict['AD_scores'], bins=50, histtype='step', density=True, label=tag)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'{plots_path}/AD_scores_histograms.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c0f1d-49fd-4659-a25c-80816612e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "    \n",
    "bkg_scores = datasets['bkg_test']['AD_scores']\n",
    "\n",
    "for tag, data_dict in datasets.items():\n",
    "    if 'bkg' in tag:\n",
    "        continue\n",
    "\n",
    "    sig_scores = data_dict['AD_scores']\n",
    "    combined_scores = np.concatenate([bkg_scores, sig_scores], axis=0)\n",
    "    combined_labels = np.concatenate([np.zeros_like(bkg_scores), np.ones_like(sig_scores)], axis=0)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(combined_labels, combined_scores)\n",
    "    auroc = auc(fpr, tpr)\n",
    "\n",
    "    plt.plot(fpr, tpr, label=f'{tag} (auc = {auroc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.annotate('better', xy=(0.7, 0.3), xytext=(0.5, 0.1),\n",
    "                textcoords='axes fraction', fontsize=12, color='red',\n",
    "                arrowprops=dict(facecolor='red', shrink=0.05, width=2, headwidth=8))\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'{plots_path}/roc_curves.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
